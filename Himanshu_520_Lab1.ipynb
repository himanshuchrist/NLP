{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d6c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello everyone, my name is Himanshu Gulechha. My E-mail ID is gulechhah@gmail.com! I wont't be going for the air show (I don't know to how to fly a plane). ‚úàÔ∏èüòé ‚ùå\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac55c39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello everyone, my name is Himanshu Gulechha. My E-mail ID is gulechhah@gmail.com! I wont't be going for the air show (I don't know to how to fly a plane). ‚úàÔ∏èüòé ‚ùå\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb213371",
   "metadata": {},
   "source": [
    "## Word tokenization is the process of splitting a text into individual words. \n",
    "### Word tokenization can be straightforward for languages with spaces between words but can be more complex for languages like Chinese or Thai that don't use spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b41bd7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Himanshu',\n",
       " 'Gulechha',\n",
       " '.',\n",
       " 'My',\n",
       " 'E-mail',\n",
       " 'ID',\n",
       " 'is',\n",
       " 'gulechhah',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " '!',\n",
       " 'I',\n",
       " \"wont't\",\n",
       " 'be',\n",
       " 'going',\n",
       " 'for',\n",
       " 'the',\n",
       " 'air',\n",
       " 'show',\n",
       " '(',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'to',\n",
       " 'how',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'a',\n",
       " 'plane',\n",
       " ')',\n",
       " '.',\n",
       " '‚úàÔ∏èüòé',\n",
       " '‚ùå']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60486e9",
   "metadata": {},
   "source": [
    "## Sentence tokenization involves splitting a text into individual sentences. \n",
    "### Essential for tasks like text summarization, machine translation, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d2cdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone, my name is Himanshu Gulechha.',\n",
       " 'My E-mail ID is gulechhah@gmail.com!',\n",
       " \"I wont't be going for the air show (I don't know to how to fly a plane).\",\n",
       " '‚úàÔ∏èüòé ‚ùå']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f4dc9",
   "metadata": {},
   "source": [
    "## Punctuation-based Tokenizer splits text based on punctuation marks such as periods, commas, and semicolons. \n",
    "### Suitable for simple text processing tasks where accuracy is not critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "363a0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Himanshu',\n",
       " 'Gulechha',\n",
       " '.',\n",
       " 'My',\n",
       " 'E',\n",
       " '-',\n",
       " 'mail',\n",
       " 'ID',\n",
       " 'is',\n",
       " 'gulechhah',\n",
       " '@',\n",
       " 'gmail',\n",
       " '.',\n",
       " 'com',\n",
       " '!',\n",
       " 'I',\n",
       " 'wont',\n",
       " \"'\",\n",
       " 't',\n",
       " 'be',\n",
       " 'going',\n",
       " 'for',\n",
       " 'the',\n",
       " 'air',\n",
       " 'show',\n",
       " '(',\n",
       " 'I',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'know',\n",
       " 'to',\n",
       " 'how',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'a',\n",
       " 'plane',\n",
       " ').',\n",
       " '‚úàÔ∏èüòé',\n",
       " '‚ùå']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc72e50",
   "metadata": {},
   "source": [
    "## The Treebank Word Tokenizer is based on the Penn Treebank corpus and uses regular expressions to tokenize text according to conventions used in the corpus.\n",
    "### Beneficial for tasks requiring fine-grained tokenization, such as syntactic parsing and part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6615c8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Himanshu',\n",
       " 'Gulechha.',\n",
       " 'My',\n",
       " 'E-mail',\n",
       " 'ID',\n",
       " 'is',\n",
       " 'gulechhah',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " '!',\n",
       " 'I',\n",
       " \"wont't\",\n",
       " 'be',\n",
       " 'going',\n",
       " 'for',\n",
       " 'the',\n",
       " 'air',\n",
       " 'show',\n",
       " '(',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'to',\n",
       " 'how',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'a',\n",
       " 'plane',\n",
       " ')',\n",
       " '.',\n",
       " '‚úàÔ∏èüòé',\n",
       " '‚ùå']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "TreebankWordTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a15b3",
   "metadata": {},
   "source": [
    "## Tweet Tokenizer is designed specifically for tokenizing tweets, which often contain unique characteristics such as hashtags, mentions, and emojis.\n",
    "###  Useful for sentiment analysis, trend analysis, and social media monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07b10469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Himanshu',\n",
       " 'Gulechha',\n",
       " '.',\n",
       " 'My',\n",
       " 'E-mail',\n",
       " 'ID',\n",
       " 'is',\n",
       " 'gulechhah@gmail.com',\n",
       " '!',\n",
       " 'I',\n",
       " \"wont't\",\n",
       " 'be',\n",
       " 'going',\n",
       " 'for',\n",
       " 'the',\n",
       " 'air',\n",
       " 'show',\n",
       " '(',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'to',\n",
       " 'how',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'a',\n",
       " 'plane',\n",
       " ')',\n",
       " '.',\n",
       " '‚úà',\n",
       " 'Ô∏è',\n",
       " 'üòé',\n",
       " '‚ùå']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "TweetTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafdee2",
   "metadata": {},
   "source": [
    "## Multi-Word Expression Tokenizer identifies and tokenizes multi-word expressions (MWEs), such as \"New York\" or \"machine learning,\" as single units.\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4650a337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Himanshu',\n",
       " 'Gulechha',\n",
       " '.',\n",
       " 'My',\n",
       " 'E-mail',\n",
       " 'ID',\n",
       " 'is',\n",
       " 'gulechhah',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " '!',\n",
       " 'I',\n",
       " \"wont't\",\n",
       " 'be',\n",
       " 'going',\n",
       " 'for',\n",
       " 'the',\n",
       " 'air',\n",
       " 'show',\n",
       " '(',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'to',\n",
       " 'how',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'a',\n",
       " 'plane',\n",
       " ')',\n",
       " '.',\n",
       " '‚úàÔ∏èüòé',\n",
       " '‚ùå']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "MWETokenizer().add_mwe(('Himanshu','Gulechha'))\n",
    "MWETokenizer().tokenize(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d92e0",
   "metadata": {},
   "source": [
    "## TextBlob library tokenizer splits text into words using the Word class's tokenization method.\n",
    "### Suitable for rapid prototyping, educational purposes, and simple NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2167ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', 'my', 'name', 'is', 'Himanshu', 'Gulechha', 'My', 'E-mail', 'ID', 'is', 'gulechhah', 'gmail.com', 'I', \"wont't\", 'be', 'going', 'for', 'the', 'air', 'show', 'I', 'do', \"n't\", 'know', 'to', 'how', 'to', 'fly', 'a', 'plane', '‚úàÔ∏èüòé', '‚ùå']\n",
      "[Sentence(\"Hello everyone, my name is Himanshu Gulechha.\"), Sentence(\"My E-mail ID is gulechhah@gmail.com!\"), Sentence(\"I wont't be going for the air show (I don't know to how to fly a plane).\"), Sentence(\"‚úàÔ∏èüòé ‚ùå\")]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "print(TextBlob(text).words)\n",
    "print(TextBlob(text).sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b1aad",
   "metadata": {},
   "source": [
    "## spaCy's tokenizer is highly customizable and designed for efficiency and accuracy. \n",
    "### Ideal for various NLP tasks, including named entity recognition, dependency parsing, and text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ddbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "everyone\n",
      ",\n",
      "my\n",
      "name\n",
      "is\n",
      "Himanshu\n",
      "Gulechha\n",
      ".\n",
      "My\n",
      "E\n",
      "-\n",
      "mail\n",
      "ID\n",
      "is\n",
      "gulechhah@gmail.com\n",
      "!\n",
      "I\n",
      "wont't\n",
      "be\n",
      "going\n",
      "for\n",
      "the\n",
      "air\n",
      "show\n",
      "(\n",
      "I\n",
      "do\n",
      "n't\n",
      "know\n",
      "to\n",
      "how\n",
      "to\n",
      "fly\n",
      "a\n",
      "plane\n",
      ")\n",
      ".\n",
      "‚úà\n",
      "Ô∏è\n",
      "üòé\n",
      "‚ùå\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "doc=spacy.blank(\"en\")(text)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70491d",
   "metadata": {},
   "source": [
    "## Gensim offers a simple word tokenizer as part of its toolkit for topic modeling and natural language processing.\n",
    "### Well-suited for tasks like topic modeling, document similarity analysis, and word embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0457365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', 'my', 'name', 'is', 'Himanshu', 'Gulechha', 'My', 'E', 'mail', 'ID', 'is', 'gulechhah', 'gmail', 'com', 'I', 'wont', 't', 'be', 'going', 'for', 'the', 'air', 'show', 'I', 'don', 't', 'know', 'to', 'how', 'to', 'fly', 'a', 'plane']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "tokens = list(tokenize(text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ca0d7",
   "metadata": {},
   "source": [
    "## Keras, a deep learning library, provides tokenization utilities for preparing text data for neural network models. \n",
    "### Essential for building deep learning models for tasks like text classification, sequence generation, and language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdb3b6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 1,\n",
       " 'h': 2,\n",
       " 'e': 3,\n",
       " 'i': 4,\n",
       " 'a': 5,\n",
       " 'l': 6,\n",
       " 'n': 7,\n",
       " 'm': 8,\n",
       " 't': 9,\n",
       " 'g': 10,\n",
       " 'y': 11,\n",
       " 's': 12,\n",
       " 'w': 13,\n",
       " 'r': 14,\n",
       " 'u': 15,\n",
       " 'c': 16,\n",
       " 'd': 17,\n",
       " \"'\": 18,\n",
       " 'f': 19,\n",
       " 'v': 20,\n",
       " 'b': 21,\n",
       " 'k': 22,\n",
       " 'p': 23,\n",
       " '‚úà': 24,\n",
       " 'Ô∏è': 25,\n",
       " 'üòé': 26,\n",
       " '‚ùå': 27}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63699ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
