{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80adb374",
   "metadata": {},
   "source": [
    "### Antonyms from WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324b2044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonym of  good  is  evil\n",
      "Antonym of  good  is  evilness\n",
      "Antonym of  good  is  bad\n",
      "Antonym of  good  is  badness\n",
      "Antonym of  good  is  bad\n",
      "Antonym of  good  is  evil\n",
      "Antonym of  good  is  ill\n",
      "Antonym of  bad  is  good\n",
      "Antonym of  bad  is  goodness\n",
      "Antonym of  bad  is  good\n",
      "Antonym of  bad  is  unregretful\n",
      "Antonym of  big  is  small\n",
      "Antonym of  big  is  little\n",
      "Antonym of  big  is  small\n",
      "Antonym of  small  is  large\n",
      "Antonym of  small  is  big\n",
      "Antonym of  small  is  big\n",
      "Antonym of  worse  is  better\n",
      "Antonym of  worse  is  better\n",
      "Antonym of  worse  is  good\n",
      "Antonym of  worse  is  unregretful\n"
     ]
    }
   ],
   "source": [
    "words=['good','bad','big','small','worse']\n",
    "from nltk.corpus import wordnet\n",
    "for i in words:\n",
    "    for syn in wordnet.synsets(i): \n",
    "        for l in syn.lemmas():\n",
    "            if l.antonyms(): \n",
    "                print(\"Antonym of \",i,\" is \",l.antonyms()[0].name()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c586d1",
   "metadata": {},
   "source": [
    "### Stemming non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a93fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schreiben  post stemming is  schreib\n",
      "geschrieben  post stemming is  geschrieb\n",
      "Hunde  post stemming is  hund\n",
      "Pferde  post stemming is  pferd\n",
      "Männer  post stemming is  mann\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import GermanStemmer\n",
    "gst = GermanStemmer()\n",
    "german=['Schreiben','geschrieben','Hunde','Pferde','Männer']\n",
    "for i in german:\n",
    "    print(i,\" post stemming is \",gst.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238d609",
   "metadata": {},
   "source": [
    "### Differentiate between stemming and lemmatizing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976102ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples  after WordnetLemmatizer- apple\n",
      "apples  after PortStemmer- appl\n",
      "apples  after LancasterStemmer- appl\n",
      "apples  after Snowball Stemmer- appl\n",
      "ladies  after WordnetLemmatizer- lady\n",
      "ladies  after PortStemmer- ladi\n",
      "ladies  after LancasterStemmer- lady\n",
      "ladies  after Snowball Stemmer- ladi\n",
      "words  after WordnetLemmatizer- word\n",
      "words  after PortStemmer- word\n",
      "words  after LancasterStemmer- word\n",
      "words  after Snowball Stemmer- word\n",
      "kicks  after WordnetLemmatizer- kick\n",
      "kicks  after PortStemmer- kick\n",
      "kicks  after LancasterStemmer- kick\n",
      "kicks  after Snowball Stemmer- kick\n",
      "games  after WordnetLemmatizer- game\n",
      "games  after PortStemmer- game\n",
      "games  after LancasterStemmer- gam\n",
      "games  after Snowball Stemmer- game\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "words=['apples','ladies','words','kicks','games']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "for i in words:\n",
    "    print(i,\" after WordnetLemmatizer-\",lemmatizer.lemmatize(i))\n",
    "    print(i,\" after PortStemmer-\",porter.stem(i))\n",
    "    print(i,\" after LancasterStemmer-\",lancaster.stem(i))\n",
    "    print(i,\" after Snowball Stemmer-\",snowball.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d653d",
   "metadata": {},
   "source": [
    "### PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae1eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$')]\n",
      "[('name', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('Himanshu', 'NN')]\n",
      "[('Gulechha', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('am', 'VBP')]\n",
      "[('24', 'CD')]\n",
      "[('years', 'NNS')]\n",
      "[('old', 'JJ')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('am', 'VBP')]\n",
      "[('the', 'DT')]\n",
      "[('best', 'JJS')]\n",
      "[('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "text='My name is Himanshu Gulechha. I am 24 years old. I am the best!'\n",
    "from nltk import pos_tag, word_tokenize\n",
    "for i in word_tokenize(text):\n",
    "    print(pos_tag(word_tokenize(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3114b7",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5145afc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Himanshu Gulechha\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "entity=ne_chunk(pos_tag(word_tokenize(text)))\n",
    "for chunk in entity:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bb21d",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "099f16de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/p1thpgyj6yv2d7xmn803p8tw0000gn/T/ipykernel_15317/3279072660.py:4: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-parser\\.jar jar file at /content/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m jar_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m models_jar_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/stanford-corenlp-4.2.2-models-english.jar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mStanfordDependencyParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_jar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_models_jar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodels_jar_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mraw_parse(text)\n\u001b[1;32m      6\u001b[0m dependency \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/nltk/parse/stanford.py:401\u001b[0m, in \u001b[0;36mStanfordDependencyParser.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    394\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe StanfordDependencyParser will be deprecated\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use \u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    399\u001b[0m     )\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/nltk/parse/stanford.py:50\u001b[0m, in \u001b[0;36mGenericStanfordParser.__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     40\u001b[0m     path_to_jar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# find the most recent code and model jar\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     stanford_jar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfind_jar_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_JAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_to_jar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSTANFORD_PARSER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSTANFORD_CORENLP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43msearchpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stanford_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     model_jar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m     64\u001b[0m         find_jar_iter(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_MODEL_JAR_PATTERN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m model_path: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(model_path),\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# self._classpath = (stanford_jar, model_jar)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Adding logging jar files to classpath\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/nltk/internals.py:734\u001b[0m, in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m path_to_jar\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m jar file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_jar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m         )\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Check environment variables\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_var \u001b[38;5;129;01min\u001b[39;00m env_vars:\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at /content/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "jar_path = '/content/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar'\n",
    "models_jar_path = '/content/stanford-corenlp-4.2.2-models-english.jar'\n",
    "parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n",
    "result = parser.raw_parse(text)\n",
    "dependency = result.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4e18510",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanfordnlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanfordnlp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanfordnlp'"
     ]
    }
   ],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892c85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
